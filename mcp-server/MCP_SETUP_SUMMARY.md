# MCP Server Configuration Summary

## âœ… Cáº¥u hÃ¬nh Ä‘Ã£ hoÃ n thÃ nh

### 1. **Qwen Model Integration** (Local - MIá»„N PHÃ)
- **Model**: `qwen3-lexi` (8.2B parameters, Q4_K_M quantization)
- **Provider**: Ollama (cháº¡y local trÃªn mÃ¡y báº¡n)
- **Status**: âœ… Configured vÃ  ready
- **Chi phÃ­**: **KHÃ”NG máº¥t phÃ­** - hoÃ n toÃ n miá»…n phÃ­
- **Location**: `/Users/nguyenhuuthang/Documents/RepoGitHub/LexiLingo/ai-service/models/qwen3/`

### 2. **Gemini Fallback** (Cloud API - CÃ“ PHÃ)
- **Model**: `gemini-1.5-flash`
- **Provider**: Google Cloud API
- **Status**: âœ… Configured as fallback only
- **Chi phÃ­**: 
  - Input: ~$0.075 / 1M tokens
  - Output: ~$0.30 / 1M tokens
  - Free tier: 15 req/min, 1M tokens/day

### 3. **Priority & Fallback Logic** âœ…
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   User Request                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Try Qwen (Local - Free)           â”‚
â”‚   - Model: qwen3-lexi               â”‚
â”‚   - Via: Ollama API                 â”‚
â”‚   - Timeout: 120s                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€ Success â†’ Return response
               â”‚
               â””â”€ Failed â†“
                         
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   âš ï¸  WARNING TRIGGERED              â”‚
â”‚   "QWEN MODEL UNAVAILABLE"          â”‚
â”‚   "Fix: Run 'ollama serve'"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Fallback to Gemini (Cloud - Paid) â”‚
â”‚   âš ï¸  "Using Gemini API fallback"    â”‚
â”‚   âš ï¸  "(may incur costs)"            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â”œâ”€ Success â†’ Return response
               â”‚
               â””â”€ Failed â†’ Error (both failed)
```

## ğŸ“‹ CÃ¡c thay Ä‘á»•i chÃ­nh

### 1. Config File ([mcp-server/config.yaml](mcp-server/config.yaml))
```yaml
models:
  qwen:
    provider: "ollama"
    model: "qwen3-lexi"
    base_url: "http://localhost:11434"
    timeout: 120  # First load needs time for 5GB model
    
  gemini:
    provider: "api"
    fallback_only: true  # Only use when Qwen fails

features:
  enable_fallback: true
  warn_on_fallback: true  # âš ï¸  Cáº£nh bÃ¡o khi dÃ¹ng fallback
```

### 2. QwenHandler ([mcp-server/handlers/qwen.py](mcp-server/handlers/qwen.py))
- âœ… Integrate vá»›i Ollama API qua httpx
- âœ… Retry logic (2 attempts)
- âœ… Connection testing khi khá»Ÿi táº¡o
- âœ… Model availability verification

### 3. Chat Tool ([mcp-server/tools/chat.py](mcp-server/tools/chat.py))
- âœ… Priority: Qwen first, Gemini fallback
- âœ… Warning messages khi Qwen fail:
  ```
  âš ï¸  QWEN MODEL UNAVAILABLE: [error]
  âš ï¸  Reason: Ollama might not be running
  âš ï¸  Fix: Run 'ollama serve' and ensure 'qwen3-lexi' is available
  ```
- âœ… Warning khi dÃ¹ng fallback:
  ```
  âš ï¸  Using Gemini API fallback (may incur costs)
  ```

### 4. Dependencies ([mcp-server/requirements.txt](mcp-server/requirements.txt))
- âœ… Added `httpx>=0.25.0` for Ollama API calls

## ğŸ¯ CÃ¡ch sá»­ dá»¥ng

### Start MCP Server
```bash
cd mcp-server
python server.py
```

### Chat vá»›i MCP (auto priority)
```python
# Tá»± Ä‘á»™ng thá»­ Qwen trÆ°á»›c, fallback Gemini náº¿u cáº§n
{
    "tool": "chat_with_ai",
    "args": {
        "message": "What is the difference between affect and effect?",
        "context": {
            "user_level": "B2",
            "session_id": "abc123"
        }
        # model khÃ´ng cáº§n chá»‰ Ä‘á»‹nh, máº·c Ä‘á»‹nh dÃ¹ng qwen
    }
}
```

### Force model cá»¥ thá»ƒ
```python
# Force dÃ¹ng Qwen (khÃ´ng fallback)
{"args": {"message": "...", "model": "qwen"}}

# Force dÃ¹ng Gemini
{"args": {"message": "...", "model": "gemini"}}
```

## âš™ï¸ YÃªu cáº§u há»‡ thá»‘ng

### Qwen (Local)
- âœ… Ollama Ä‘Ã£ cÃ i Ä‘áº·t vÃ  running
- âœ… Model `qwen3-lexi` Ä‘Ã£ load: `ollama list`
- âœ… Service running: `ps aux | grep ollama`
- âœ… RAM: ~6-8GB khi model Ä‘Æ°á»£c load

### Gemini (Fallback)
- âœ… API Key: `GEMINI_API_KEY` environment variable
- âœ… Internet connection

## ğŸ” Kiá»ƒm tra tráº¡ng thÃ¡i

### 1. Check Ollama
```bash
# Check service
ps aux | grep ollama

# Check models
ollama list

# Test API
curl http://localhost:11434/api/tags
```

### 2. Check MCP Config
```bash
cd mcp-server
python -c "from utils.config import Config; c = Config.load('config.yaml'); print(c.get('models.qwen'))"
```

### 3. Test Qwen Handler
```bash
cd mcp-server
python test_qwen_handler.py
```

## ğŸ› Troubleshooting

### Váº¥n Ä‘á»: Qwen timeout
**NguyÃªn nhÃ¢n**: Láº§n Ä‘áº§u tiÃªn model cáº§n load vÃ o memory (5.2GB)
**Giáº£i phÃ¡p**: 
- Äá»£i ~30-60s cho láº§n Ä‘áº§u
- TÄƒng timeout trong config (Ä‘Ã£ set 120s)
- Hoáº·c pre-load model: `ollama run qwen3-lexi "test"`

### Váº¥n Ä‘á»: Ollama not responding
**Kiá»ƒm tra**:
```bash
ollama serve  # Start náº¿u chÆ°a cháº¡y
ollama ps     # Check running models
```

### Váº¥n Ä‘á»: Gemini fallback khÃ´ng work
**Kiá»ƒm tra**:
```bash
echo $GEMINI_API_KEY  # Check API key
```

## ğŸ“Š Chi phÃ­ dá»± kiáº¿n

### Qwen (Recommended)
- **Chi phÃ­**: $0 (hoÃ n toÃ n miá»…n phÃ­)
- **Tá»‘c Ä‘á»™**: ~2-5s/response (sau khi loaded)
- **RAM**: ~6-8GB

### Gemini (Fallback Emergency)
- **Chi phÃ­**: 
  - Trong free tier: $0 (giá»›i háº¡n 1M tokens/day)
  - NgoÃ i free tier: ~$0.10-0.50 per 1000 responses
- **Tá»‘c Ä‘á»™**: ~1-3s/response
- **RAM**: Minimal (cloud API)

## âœ… Káº¿t luáº­n

**MCP Server Ä‘Ã£ sáºµn sÃ ng** vá»›i:
1. âœ… Qwen local model Æ°u tiÃªn (MIá»„N PHÃ)
2. âœ… Gemini fallback chá»‰ khi cáº§n (CÃ“ PHÃ)
3. âœ… Cáº£nh bÃ¡o rÃµ rÃ ng khi cÃ³ váº¥n Ä‘á»
4. âœ… Retry logic vÃ  error handling
5. âœ… Config linh hoáº¡t vÃ  dá»… customize

**Khuyáº¿n nghá»‹**: Giá»¯ Ollama cháº¡y liÃªn tá»¥c Ä‘á»ƒ trÃ¡nh pháº£i fallback sang Gemini (cÃ³ phÃ­).
