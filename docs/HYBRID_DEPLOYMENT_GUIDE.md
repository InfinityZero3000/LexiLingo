# üöÄ Hybrid Deployment Guide - LexiLingo

> **Chi·∫øn l∆∞·ª£c:** AI Service ch·∫°y local tr√™n m√°y b·∫°n, c√≤n l·∫°i deploy mi·ªÖn ph√≠ tr√™n cloud

---

## üìä Ki·∫øn tr√∫c Hybrid

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    INTERNET (Public Access)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                  ‚îÇ                  ‚îÇ
        ‚ñº                  ‚ñº                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Flutter Web  ‚îÇ   ‚îÇ Web Admin    ‚îÇ   ‚îÇBackend Service‚îÇ
‚îÇ   (Vercel)   ‚îÇ   ‚îÇ  (Netlify)   ‚îÇ   ‚îÇ (Render.com) ‚îÇ
‚îÇ   FREE       ‚îÇ   ‚îÇ   FREE       ‚îÇ   ‚îÇ    FREE      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚îÇ
                                              ‚ñº
                                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                      ‚îÇ  PostgreSQL  ‚îÇ
                                      ‚îÇ (Supabase)   ‚îÇ
                                      ‚îÇ    FREE      ‚îÇ
                                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                              ‚ñ≤
                                              ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ
        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  YOUR LOCAL MACHINE (Private)                     ‚îÇ
‚îÇ                                                                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ            AI Service (localhost:8001)                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Whisper STT (244MB)                                     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Qwen NLP (900MB)                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Piper TTS (63MB)                                        ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Knowledge Graph (50MB)                                  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  Total RAM: ~2.4GB                                         ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                           ‚ñ≤                                       ‚îÇ
‚îÇ                           ‚îÇ                                       ‚îÇ
‚îÇ                  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                              ‚îÇ
‚îÇ                  ‚îÇ  Tunnel Service ‚îÇ                              ‚îÇ
‚îÇ                  ‚îÇ  (Expose to     ‚îÇ                              ‚îÇ
‚îÇ                  ‚îÇ   Internet)     ‚îÇ                              ‚îÇ
‚îÇ                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                              ‚îÇ
‚îÇ                     ‚Ä¢ Ngrok (Free)                                ‚îÇ
‚îÇ                     ‚Ä¢ Cloudflare Tunnel (Free)                    ‚îÇ
‚îÇ                     ‚Ä¢ Tailscale Funnel (Free)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ ∆Øu ƒëi·ªÉm c·ªßa Hybrid Setup

| Aspect | Traditional Cloud | Hybrid Setup |
|--------|------------------|--------------|
| **Chi ph√≠ AI Service** | $50-100/month (GPU) | $0 (d√πng m√°y local) |
| **Chi ph√≠ t·ªïng** | $50-100/month | $0 (ho√†n to√†n mi·ªÖn ph√≠!) |
| **GPU Performance** | Limited (free tier) | Full GPU c·ªßa b·∫°n |
| **RAM cho AI** | 512MB-1GB | T√πy m√°y (4-16GB) |
| **Cold start** | 30-60s | 0s (always on) |
| **Latency** | Cloud ‚Üí Cloud | Cloud ‚Üí Home (th√™m ~50ms) |
| **Scalability** | Auto-scale | Manual/Docker scale |

---

## üîß Setup Steps

### **STEP 1: Setup Tunnel cho AI Service**

#### Option A: Cloudflare Tunnel (KHUY·∫æN NGH·ªä ‚≠ê)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Ho√†n to√†n mi·ªÖn ph√≠, kh√¥ng gi·ªõi h·∫°n bandwidth
- ‚úÖ T·ª± ƒë·ªông SSL/TLS
- ‚úÖ DDoS protection built-in
- ‚úÖ Kh√¥ng c·∫ßn m·ªü port router
- ‚úÖ Static domain mi·ªÖn ph√≠ (*.trycloudflare.com ho·∫∑c custom)

**Setup:**

```bash
# 1. Install cloudflared
brew install cloudflare/cloudflare/cloudflared  # macOS
# ho·∫∑c https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/install-and-setup/installation/

# 2. Login (l·∫ßn ƒë·∫ßu)
cloudflared tunnel login

# 3. Create tunnel
cloudflared tunnel create lexilingo-ai

# 4. Configure tunnel (t·∫°o file config)
mkdir -p ~/.cloudflared
cat > ~/.cloudflared/config.yml << 'EOF'
tunnel: lexilingo-ai
credentials-file: ~/.cloudflared/<TUNNEL-ID>.json

ingress:
  - hostname: ai.yourdomain.com  # Ho·∫∑c d√πng *.trycloudflare.com
    service: http://localhost:8001
  - service: http_status:404
EOF

# 5. Route DNS (n·∫øu d√πng custom domain)
cloudflared tunnel route dns lexilingo-ai ai.yourdomain.com

# 6. Run tunnel
cloudflared tunnel run lexilingo-ai
```

**Quick Start (kh√¥ng c·∫ßn account):**
```bash
# T·∫°o temporary public URL ngay l·∫≠p t·ª©c
cloudflared tunnel --url http://localhost:8001

# Output: https://random-subdomain.trycloudflare.com
# Copy URL n√†y ƒë·ªÉ config v√†o backend service
```

---

#### Option B: Ngrok (ƒê∆°n gi·∫£n nh·∫•t)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ C·ª±c k·ª≥ ƒë∆°n gi·∫£n
- ‚úÖ Free tier: 1 tunnel, 40 connections/minute

**H·∫°n ch·∫ø:**
- ‚ö†Ô∏è Random URL m·ªói l·∫ßn restart
- ‚ö†Ô∏è Rate limit 40 req/min

**Setup:**
```bash
# 1. Install
brew install ngrok  # macOS

# 2. Sign up & get auth token
ngrok config add-authtoken <YOUR_TOKEN>

# 3. Expose AI service
ngrok http 8001

# Output:
# Forwarding: https://abc123.ngrok.io -> localhost:8001
```

---

#### Option C: Tailscale Funnel (B·∫£o m·∫≠t cao)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ Mi·ªÖn ph√≠, kh√¥ng rate limit
- ‚úÖ VPN built-in (ch·ªâ authorized users access)
- ‚úÖ No public exposure (secure)

**Setup:**
```bash
# 1. Install
brew install tailscale

# 2. Login
tailscale up

# 3. Enable funnel
tailscale funnel 8001

# Output: https://your-machine.tailnet.ts.net
```

---

### **STEP 2: Deploy Backend Service l√™n Render.com**

**File: `render.yaml`** (ƒë√£ t·∫°o t·ª± ƒë·ªông ·ªü b∆∞·ªõc sau)

```yaml
services:
  - type: web
    name: lexilingo-backend
    env: python
    region: singapore  # Ch·ªçn g·∫ßn VN
    plan: free
    buildCommand: "pip install -r requirements.txt"
    startCommand: "uvicorn app.main:app --host 0.0.0.0 --port $PORT"
    envVars:
      - key: DATABASE_URL
        sync: false  # Nh·∫≠p manual t·ª´ Supabase
      - key: AI_SERVICE_URL
        value: https://your-tunnel-url.trycloudflare.com  # T·ª´ Step 1
      - key: JWT_SECRET_KEY
        generateValue: true
      - key: FIREBASE_SERVICE_ACCOUNT
        sync: false
```

---

### **STEP 3: Setup Database tr√™n Supabase**

1. V√†o https://supabase.com/dashboard
2. T·∫°o project m·ªõi: "lexilingo"
3. Copy Connection String:
   ```
   postgresql://postgres:[password]@db.xxx.supabase.co:5432/postgres
   ```
4. Ch·∫°y migrations:
   ```bash
   # C·∫≠p nh·∫≠t DATABASE_URL trong .env
   export DATABASE_URL="postgresql://..."
   
   cd backend-service
   alembic upgrade head
   ```

---

### **STEP 4: Deploy Flutter Web l√™n Vercel**

**File: `vercel.json`** (ƒë√£ t·∫°o ·ªü flutter-app/)

```json
{
  "buildCommand": "flutter build web --release",
  "outputDirectory": "build/web",
  "framework": null,
  "rewrites": [
    {
      "source": "/(.*)",
      "destination": "/index.html"
    }
  ],
  "env": {
    "BACKEND_URL": "https://lexilingo-backend.onrender.com",
    "AI_SERVICE_URL": "https://your-tunnel-url.trycloudflare.com"
  }
}
```

**Deploy:**
```bash
cd flutter-app
flutter build web --release

# Push to GitHub, then:
# 1. Import repo v√†o Vercel
# 2. Set build command: "flutter build web"
# 3. Set output dir: "build/web"
```

---

### **STEP 5: Deploy Web Admin l√™n Netlify**

**File: `netlify.toml`** (ƒë√£ t·∫°o ·ªü web-admin/)

```toml
[build]
  command = "npm run build"
  publish = "dist"

[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200

[context.production.environment]
  VITE_BACKEND_URL = "https://lexilingo-backend.onrender.com"
```

---

## üê≥ Docker Setup cho AI Service (Local)

**File: `docker-compose.local.yml`**

```yaml
version: '3.8'

services:
  ai-service:
    build: ./ai-service
    ports:
      - "8001:8001"
    environment:
      - GEMINI_API_KEY=${GEMINI_API_KEY}
      - MODEL_CACHE_DIR=/app/models
    volumes:
      - ./ai-service/models:/app/models  # Cache models
      - ./ai-service/data:/app/data
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
```

**Run:**
```bash
docker-compose -f docker-compose.local.yml up -d
```

---

## üîÑ Auto-restart & Monitoring

### Systemd Service (Linux/macOS)

**File: `/etc/systemd/system/lexilingo-ai.service`**

```ini
[Unit]
Description=LexiLingo AI Service with Cloudflare Tunnel
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/path/to/LexiLingo/ai-service
ExecStart=/bin/bash -c 'source /path/to/.venv/bin/activate && python -m uvicorn api.main_lite:app --host 0.0.0.0 --port 8001 & cloudflared tunnel run lexilingo-ai'
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

**Enable:**
```bash
sudo systemctl enable lexilingo-ai
sudo systemctl start lexilingo-ai
sudo systemctl status lexilingo-ai
```

---

### LaunchAgent (macOS - KHUY·∫æN NGH·ªä)

**File: `~/Library/LaunchAgents/com.lexilingo.ai.plist`**

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.lexilingo.ai</string>
    <key>ProgramArguments</key>
    <array>
        <string>/bin/bash</string>
        <string>/Users/nguyenhuuthang/Documents/RepoGitHub/LexiLingo/scripts/start-ai-local.sh</string>
    </array>
    <key>RunAtLoad</key>
    <true/>
    <key>KeepAlive</key>
    <true/>
    <key>StandardOutPath</key>
    <string>/Users/nguyenhuuthang/Documents/RepoGitHub/LexiLingo/logs/ai-local.log</string>
    <key>StandardErrorPath</key>
    <string>/Users/nguyenhuuthang/Documents/RepoGitHub/LexiLingo/logs/ai-local.error.log</string>
</dict>
</plist>
```

**Load:**
```bash
launchctl load ~/Library/LaunchAgents/com.lexilingo.ai.plist
launchctl start com.lexilingo.ai
```

---

## üìä Monitoring & Health Check

**Health check endpoint:**
```bash
# Check AI service
curl http://localhost:8001/health

# Check through tunnel
curl https://your-tunnel-url.trycloudflare.com/health
```

**Setup uptime monitoring (free):**
- https://uptimerobot.com (50 monitors free)
- Ping AI service m·ªói 5 ph√∫t
- Email/SMS alert n·∫øu down

---

## üîí Security Considerations

### 1. API Key Protection
```bash
# AI Service ch·ªâ accept requests t·ª´ backend
# File: ai-service/api/middleware.py

ALLOWED_ORIGINS = [
    "https://lexilingo-backend.onrender.com",
    "https://your-frontend.vercel.app"
]
```

### 2. Cloudflare Access (Optional)
- Enable Cloudflare Access rules
- Ch·ªâ allow IP c·ªßa Render.com

### 3. Rate Limiting
```python
# ai-service/api/main_lite.py
from fastapi_limiter import FastAPILimiter
from fastapi_limiter.depends import RateLimiter

@app.on_event("startup")
async def startup():
    redis = await aioredis.create_redis_pool("redis://localhost")
    await FastAPILimiter.init(redis)

@app.post("/chat")
@limiter.limit("10/minute")  # 10 requests per minute
async def chat_endpoint():
    ...
```

---

## üí∞ Chi ph√≠ Chi ti·∫øt

| Service | Platform | Monthly Cost |
|---------|----------|--------------|
| Frontend (Flutter Web) | Vercel | $0 |
| Web Admin | Netlify | $0 |
| Backend | Render.com | $0 (750h free) |
| Database | Supabase | $0 (500MB) |
| AI Service | Local Machine | $0 |
| Tunnel | Cloudflare | $0 |
| **TOTAL** | | **$0/month** üéâ |

**Electricity cost estimate:**
- AI Service running 24/7: ~100W
- Monthly: 100W √ó 24h √ó 30d = 72 kWh
- Cost (VN): 72 √ó 2,000 VND = **144,000 VND/month** (~$6)

**So s√°nh v·ªõi full cloud:**
- Full cloud with GPU: $50-100/month
- Hybrid: $6/month (ƒëi·ªán)
- **Ti·∫øt ki·ªám: $44-94/month** üí∞

---

## üö¶ Quick Start Commands

**1. Start local AI service:**
```bash
bash scripts/start-ai-local.sh
```

**2. Start tunnel:**
```bash
cloudflared tunnel --url http://localhost:8001
# Copy URL output
```

**3. Update backend config:**
```bash
# On Render.com dashboard:
# Environment Variables ‚Üí AI_SERVICE_URL ‚Üí Paste tunnel URL
```

**4. Deploy:**
```bash
git push origin main  # Auto-deploy to Vercel/Render/Netlify
```

---

## üîß Troubleshooting

### Tunnel connection failed
```bash
# Check if AI service is running
curl http://localhost:8001/health

# Restart tunnel
pkill cloudflared
cloudflared tunnel run lexilingo-ai
```

### Backend can't reach AI service
```bash
# Test from backend
curl https://your-tunnel-url.trycloudflare.com/health

# Check CORS settings in ai-service
```

### High latency
```bash
# Option 1: Use Cloudflare Argo Tunnel (faster routing)
# Option 2: Deploy AI to cloud when traffic grows
# Option 3: Use Redis cache for common requests
```

---

## üìà Scaling Strategy

**Khi n√†o c·∫ßn scale:**
- Traffic > 1000 users/day
- Response time > 3s
- M√°y local kh√¥ng ƒë·ªß m·∫°nh

**Scale options:**
1. **Vertical scaling (local):**
   - Upgrade RAM/GPU
   - Quantize models th√™m (Q4 ‚Üí Q3)

2. **Horizontal scaling:**
   - Deploy AI service l√™n cloud (Modal.com $30 credit)
   - Keep local as backup/development

3. **Hybrid caching:**
   - Cache common responses tr√™n Redis (Upstash free tier)
   - CDN for static content

---

## üìù Next Steps

- [ ] Setup Cloudflare Tunnel
- [ ] Deploy backend to Render.com
- [ ] Setup Supabase database
- [ ] Deploy frontend to Vercel
- [ ] Configure environment variables
- [ ] Test end-to-end flow
- [ ] Setup monitoring
- [ ] Enable auto-restart

**Estimated setup time:** 2-3 hours  
**Result:** $0/month deployment! üöÄ
