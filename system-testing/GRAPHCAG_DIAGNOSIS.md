# GraphCAG Performance Diagnosis

## üîç V·∫•n ƒë·ªÅ ph√°t hi·ªán

### Tri·ªáu ch·ª©ng
- GraphCAG Analysis endpoint timeout sau 180s
- Ollama kh√¥ng ph·∫£n h·ªìi requests
- GUI test tool hi·ªÉn th·ªã FAIL

### Root Cause Analysis

**1. Ollama Model Performance Issue**
```bash
PID: 26021, CPU: 473.0%, MEM: 4785.61MB
CMD: ollama runner --ollama-engine --model qwen3-lexi
```

**Ph√¢n t√≠ch:**
- Model `qwen3-lexi` (5.2GB) ƒëang chi·∫øm **473% CPU** (4-5 cores)
- RAM usage: **4.78GB** ch·ªâ cho model
- Test inference timeout sau 20-30s v·ªõi prompt ƒë∆°ngi·∫£n "Hi"

**2. Hardware Constraints**
```
Model size:    5.2 GB
RAM required:  ~6-8 GB (with overhead)
CPU cores:     ƒêang d√πng 4-5 cores ·ªü 100%
Inference time: > 30s cho 1 token ƒë∆°n gi·∫£n
```

**3. Test Results**
| Test | Status | Latency | Note |
|------|--------|---------|------|
| API Health | WARNING | 241ms | Status: ok |
| Ollama | PASS | 3ms | 2 models loaded |
| GraphCAG Endpoint | PASS | 3ms | Endpoint available |
| **GraphCAG Analysis** | **FAIL** | **Timeout (180s)** | Model kh√¥ng ph·∫£n h·ªìi |

---

## üéØ Nguy√™n nh√¢n ch√≠nh

**Ollama inference qu√° ch·∫≠m do hardware kh√¥ng ƒë·ªß m·∫°nh:**

1. **Model qu√° l·ªõn (5.2GB):** qwen3-lexi c·∫ßn > 6GB RAM + CPU m·∫°nh
2. **CPU inference:** Kh√¥ng c√≥ GPU acceleration (Apple Metal c√≥ th·ªÉ ch·∫≠m)
3. **Context loading:** Model load m·∫•t th·ªùi gian, inference c√≤n ch·∫≠m h∆°n

**B·∫±ng ch·ª©ng:**
```bash
# Test tr·ª±c ti·∫øp Ollama
$ curl -X POST http://localhost:11434/api/chat \
   -d '{"model": "qwen3-lexi", "messages": [{"role":"user","content":"Hi"}]}'

# K·∫øt qu·∫£: Timeout after 30s with 0 bytes received
```

---

## üí° Gi·∫£i ph√°p

### Option 1: S·ª≠ d·ª•ng model nh·ªè h∆°n (Recommended)
```python
# .env
OLLAMA_MODEL=qwen3:1.5b  # Nh·ªè h∆°n, nhanh h∆°n
```

Ho·∫∑c d√πng model quantized:
```bash
ollama pull qwen3:0.5b-q4_0  # 500MB, r·∫•t nhanh
```

### Option 2: TƒÉng timeout (Temporary workaround)
```python
# system-testing/graphcag_system_test.py
class TestConfig:
    timeout: int = 300  # TƒÉng t·ª´ 180s ‚Üí 300s
```

**L∆∞u √Ω:** V·∫´n s·∫Ω ch·∫≠m, kh√¥ng khuy·∫øn kh√≠ch cho production.

### Option 3: S·ª≠ d·ª•ng Gemini API (Cloud fallback)
```python
# .env
USE_OLLAMA=false
USE_GATEWAY=true  # S·∫Ω d√πng Gemini n·∫øu Ollama fail
```

Gemini API nhanh h∆°n (~2-3s) v√† kh√¥ng t·ªën t√†i nguy√™n local.

### Option 4: Upgrade hardware
- **CPU:** 8+ cores recommended
- **RAM:** 16GB+ (32GB ideal)
- **GPU:** NVIDIA GPU v·ªõi CUDA ho·∫∑c Mac M2/M3 Pro tr·ªü l√™n

---

## üß™ Verification Steps

### Test Ollama tr·ª±c ti·∫øp:
```bash
# 1. Kill stuck process
pkill -9 -follama runner

# 2. Test v·ªõi model nh·ªè
ollama run qwen3:1.5b "Say hi"

# 3. Measure latency
time curl -X POST http://localhost:11434/api/chat \
  -d '{"model":"qwen3:1.5b","messages":[{"role":"user","content":"Hi"}],"stream":false}'
```

### Test GraphCAG v·ªõi GUI tool:
1. Ch·∫°y `graphcag_system_test.py`
2. Click "Check Ollama" ‚Üí Verify PASS
3. Click "Test Ollama Inference Speed" ‚Üí Check latency
4. N·∫øu < 10s ‚Üí OK, c√≥ th·ªÉ d√πng
5. N·∫øu > 30s ‚Üí C·∫ßn ƒë·ªïi model ho·∫∑c fallback Gemini

---

## üìä Performance Expectations

### Acceptable Performance:
| Component | Target | Current | Status |
|-----------|--------|---------|--------|
| Health Check | < 100ms | 241ms | ‚ö†Ô∏è OK |
| Ollama (lists) | < 10ms | 3ms | ‚úÖ Good |
| Ollama inference | < 5s | **>30s** | ‚ùå Too slow |
| GraphCAG total | < 10s | Timeout | ‚ùå Unusable |

### V·ªõi model nh·ªè h∆°n (qwen3:1.5b):
- Expected inference: **2-3s**
- GraphCAG total: **5-8s**
- Usable cho testing v√† development

---

## üîß Quick Fix

```bash
# 1. Stop AI service
pkill -f "uvicorn api.main_lite"

# 2. ƒê·ªïi sang model nh·ªè trong .env
sed -i '' 's/OLLAMA_MODEL=qwen3-lexi/OLLAMA_MODEL=qwen3:1.5b/' ai-service/.env

# 3. Restart AI service
cd ai-service
export PYTHONPATH=$(pwd)
export GEMINI_API_KEY='your-key'
python -m uvicorn api.main_lite:app --host 0.0.0.0 --port 8001 &

# 4. Test l·∫°i v·ªõi GUI tool
cd ../system-testing
python graphcag_system_test.py
```

---

## üìù K·∫øt lu·∫≠n

**V·∫•n ƒë·ªÅ KH√îNG ph·∫£i code hay architecture, m√† l√† hardware performance.**

- ‚úÖ GraphCAG pipeline code ho·∫°t ƒë·ªông ƒë√∫ng
- ‚úÖ Ollama connection OK
- ‚úÖ Model loaded th√†nh c√¥ng
- ‚ùå **Model inference qu√° ch·∫≠m (>30s) do hardware y·∫øu**

**Khuy·∫øn ngh·ªã:**
- Development: D√πng model nh·ªè (qwen3:1.5b) ho·∫∑c Gemini API
- Production: Deploy l√™n server c√≥ GPU ho·∫∑c d√πng cloud API
